{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuan-168/sklearn/blob/main/ml_fhr_s05_missed_appts_v02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHcho1CGX25x"
      },
      "source": [
        "\n",
        "# Predicting Hospital No-Shows using Neural Networks #\n",
        "\n",
        "\n",
        "### Contents: ###\n",
        "1. Problem Description & Practical Plan\n",
        "2. Task 1: Preparation: importing and loading data\n",
        "3. Task 2: Data Preparation\n",
        "4. Task 3: SMOTE & Scale\n",
        "5. Task 4: Build and Evalute the Neural Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isCTBxNkX6Ku"
      },
      "source": [
        "## The Problem ##\n",
        "\n",
        "### Why Predict Missed Appointments? ###\n",
        "\n",
        "Approximately 20% of medical appointments that are booked are not attended in the UK. This costs an estimated £1 billion annually. If we were able to predict who might not show up, we could take targeted measures to encourage them to do so, such as tailored text alerts or a quick phone call. Let’s try and use machine learning to predict who might not show up to their appointment\n",
        "\n",
        "## Practical Plan ##\n",
        "\n",
        "Our aim is to use the **sklearn** package to Build a feed-forward Neural Network to predict no-show for hospital appointments\n",
        "\n",
        "We will use a freely-available dataset which has been downloaded from **Kaggle** (https://www.kaggle.com/datasets)\n",
        "\n",
        "### Steps: ###\n",
        "a) Load the data<br>\n",
        "b) Perform feature engineering to transform the data into an ML-digestible format<br>\n",
        "c) Define the neural network<br>\n",
        "d) Train & fit the neural network model<br>\n",
        "e) Evaluate the model & make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOUoSIiuTjFq"
      },
      "source": [
        "## Task 1: Importing and Reading the Data  ##\n",
        "\n",
        "Run the following cell to get the dataset from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89L_Jzj68NGB"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Get data from github\n",
        "#\n",
        "!wget -nv https://github.com/kcl-bhi-is-01/datasets/raw/main/MissedAppointments.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw0xP8Q78NGB"
      },
      "source": [
        "Use Pandas to read the file and examine its contents (using head(), tail(),  etc..)<br>\n",
        "\n",
        "Plot a pie chart of the outcome variable *No_show* to understand the data distribution<br>\n",
        "\n",
        "**Note:** You will need to obtain counts of the different categorical values in *No_show*, and use the `matplotlib` package, to produce the pie chart,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jazJZ-Z-TEpC",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#\n",
        "######### Your solution here ##########\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2t_MdEtYKTu"
      },
      "source": [
        "## Task 2: Data Preparation ##\n",
        "\n",
        "### Task 2.1 Renaming and dropping columns  ###\n",
        "\n",
        "The head() function reveals a few things:\n",
        "\n",
        "a) It’s clear that there’s a few spelling errors; *Hipertension* and *Handcap*. Your task is to correct that by renaming the columns<br>\n",
        "b) You can use the Pandas *rename* function. Here is the link to the documentation. It also contains useful examples: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html<br>\n",
        "c) Check that your code has worked by examining the column names (i.e. data.columns) or using head() again<br>\n",
        "d) The *PatientId* and *AppointmentID* columns are not useful as features. In fact, they shouldn't be used as features. Why do you think this is? Your task is to remove them from the dataframe. You can use the Pandas *drop* function. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html<br><br>\n",
        "      \n",
        "For Pandas *rename* and *drop* , take a note of the **inplace** argument and how it can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhHy-3XSTMBj"
      },
      "outputs": [],
      "source": [
        "#\n",
        "######### Your solution here ##########\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuhL-E1bYUmn"
      },
      "source": [
        "### Task 2.2 Feature Engineering on Appointment Date ###\n",
        "\n",
        "Use data.dtypes to see the data types of each feature in the dataframe<br>\n",
        "\n",
        "An important thing for us to do before commencing our pipeline is to set the correct data types. For this we can use the function `.apply(np.[select a datatype])`<br>\n",
        "\n",
        "For example, our columns *ScheduledDay* and *AppointmentDay* are currently **object** types but they should be **datetime** types<br>\n",
        "\n",
        "Run the next cell to convert *ScheduledDay* and *AppointmentDay* to **datetime**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epu4RiFNUJtu"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Convert ScheduledDay and AppointmentDay to datetime\n",
        "#\n",
        "import numpy as np\n",
        "#\n",
        "data.AppointmentDay = data.AppointmentDay.apply(np.datetime64)\n",
        "data.ScheduledDay   = data.ScheduledDay.apply(np.datetime64)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojb-55w9YeV2"
      },
      "source": [
        "Now that we have the columns in the right type, we can take our two features further. A factor that may play a big role in whether or not a person makes the appointment is the waiting time between scheduling and appointment. We can therefore create a new feature *WaitingTime* that calculates the difference in time between the scheduling date and appointment date. We can convert this feature into an integer feature then **drop** the actual date features!<br><br>\n",
        "\n",
        "There's a nice function in Pandas called `to_timedelta`, which finds the difference betwen two datetime objects:  https://pandas.pydata.org/docs/reference/api/pandas.to_timedelta.html. Your task is to use this function to calculate the waiting time, assigning this to a new column in your dataframe.  You can then drop the two columns *ScheduledDay* and *AppointmentDay*<br><br>\n",
        "\n",
        "Note: `to_timedelta` returns a timedelta object, which contains components for the day, time, year, etc.. We want the difference in days, so we extract that specific component, and convert it into an integer object<br><br>\n",
        "\n",
        "Run the following cell to create *WaitingTime* as the difference between *AppointmentDay* and *ScheduledDay*, after converting them to **timedelta**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei7e1e_YULxS"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Create WaitingTime column\n",
        "#\n",
        "import datetime as dt\n",
        "#\n",
        "data['WaitingTime'] = pd.to_timedelta((data['AppointmentDay'] - data['ScheduledDay'])).dt.days\n",
        "data['WaitingTime'] = data['WaitingTime'].apply(np.int64)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-4suefA8NGD"
      },
      "source": [
        "Your task is to delete the columns *AppointmentDay* and *ScheduledDay* from your data frame, using the next cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zTnS2wv8NGD"
      },
      "outputs": [],
      "source": [
        "#\n",
        "######### Your solution here ##########\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4QDtEZFYlEx"
      },
      "source": [
        "Now have a look at the values of WaitingTime. You could use `data.WaitingTime.unique()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyyLZW8-UNuB"
      },
      "outputs": [],
      "source": [
        "#\n",
        "######### Your solution here ##########\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apWH57f8Yrca"
      },
      "source": [
        "You can see that some patients have waiting time == -1. This generally designates walk-in appointments. Those rows should be removed because including such patients will skew the model. Your task is to:<br>\n",
        "\n",
        "a) Remove the rows where patients have *WaitingTime* == -1<br>\n",
        "b) Replot the distribution of the outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G18kHaOXUO6C",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#\n",
        "#### Your solution here ######\n",
        "#\n",
        "data.drop(data[data.WaitingTime == -1].index, inplace=True)\n",
        "#\n",
        "no_show_counts = data['No_show'].value_counts()\n",
        "#\n",
        "plt.pie([no_show_counts[\"Yes\"], no_show_counts[\"No\"]], labels=[\"Yes show\",\"No show\"])\n",
        "plt.title('Hospital Appts')\n",
        "plt.show()\n",
        "#\n",
        "print(data.shape)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ0v0GvQYyvD"
      },
      "source": [
        "What changes in the distribution are illustrated by the new pie chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXU3kG-iY4UH"
      },
      "source": [
        "### Task 2.3 Recoding categorical variables into integers ###\n",
        "\n",
        "Currently, the outcome column *No_show* contains `Yes` for no_show patients and `No` for patients who attended their appointments. Similarly, the *gender* column contains `F` and `M`. Moreover, the *Neighbourhood* column contains 80 unique values! (check with `data.Neighbourhood.nunique()`)<br>\n",
        "\n",
        "Machine learning (and deep learning) models, like those in **sklearn** and **Keras**, require all input and output variables to be numeric. This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model.<br>\n",
        "\n",
        "The two most popular techniques are **integer encoding** and **one-hot encoding**:<br>\n",
        "a) Integer Encoding: Where each unique label is mapped to an integer<br>\n",
        "b) One Hot Encoding: Where each label is mapped to a binary vector<br>\n",
        "\n",
        "We will use integer encoding in this practical\n",
        "\n",
        "**Integer (ordinal) encoding**<br>\n",
        "\n",
        "You could use the Pandas `map` function to transform your categorical variables, But this is not the most efficient and generalisable way<br>\n",
        "\n",
        "The **scikit-learn** package provides a class that encode categorical variables as integers, `LabelEncoder()`\n",
        "(https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)<br>\n",
        "\n",
        "The correct way to encode data in a machine learning task, is to learn the encoding from a training data set and then apply it independently to the training set and the test set. Why do you think this is? What if a categorical label should be in the test set but not in the training set?<br>\n",
        "\n",
        "**Your tasks** are to:<br>\n",
        "a) Separate the dataset into *X*, containing all columns **except** the outcome value (*No_show*), and *y*, containing only the values in *No_show*<br>\n",
        "b) Split *X* and *y* into *X_train*, *X_test*, and *y_train*, *y_test* using `train_test_split()`, from the `sklearn.model_selection` module. Assign 33% of the data to the test sets, and the remainder to training<br>\n",
        "\n",
        "Note that the separation into *X* and *y* is because many other functions we will be using further on require this arangement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2WbWW248NGE"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#### Your solution here ####\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-df6mxBH8NGE"
      },
      "source": [
        "The function given below, `label_encoder()`, takes a column from the training set, expects the same column from the test set, and generates an integer encoder model. The encoder is then applied to the training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbHGp08o8NGE"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Run this cell to define the function label_encoder()\n",
        "#\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#\n",
        "def label_encoder(train, test):\n",
        "    #\n",
        "    le = LabelEncoder()\n",
        "    #\n",
        "    le.fit(train)\n",
        "    #\n",
        "    train_enc = le.transform(train)\n",
        "    test_enc  = le.transform(test)\n",
        "    #\n",
        "    return train_enc, test_enc\n",
        "#\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4cYiJUS8NGE"
      },
      "source": [
        "**Your task** is to use `label_encoder()` to re-code the categorical variables in `y_train`, `y_test`, `X_train`, and `X_test`,  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtc7qBuw8NGE"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#### Your solution here ####\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bODRXhsaaGMC"
      },
      "source": [
        "\n",
        "## Task 3: Scaling & Preparation for Imbalanced Classification  ##\n",
        "\n",
        "The pie charts have shown us that the dataset has a class inbalance<br>\n",
        "\n",
        "We can use *Synthetic Minority Oversampling TEchnique* (SMOTE) to balance the training dataset and thus optimise model performance (https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)<br>\n",
        "\n",
        "We ensure that our feature columns all scaled the same, so that they do not unfairly influence the model due to their relative magnitudes. Remember that the weighted sum, prior to activation, is sensitive to the magnitute of **x**<br><br>\n",
        "Use the `StandardScaler` `fit()` and `transform()` functions from the `preprocessing` module in `sklearn` to apply normalized scaling\n",
        "\n",
        "**Your** task is to:<br>\n",
        "\n",
        "a) Use SMOTE from the *imblearn* package to *oversample* the **training data ONLY**<br>\n",
        "b) Scale the dataset by fitting the scaler to the SMOTed training data, then using the `transform()` function to transform both training and test sets<br><br>\n",
        "\n",
        "Why do you think we don't want to apply SMOTE to the test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f2OPCJ7aJDB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#\n",
        "##### Your solution here #####\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4nuJyPSfdp_"
      },
      "source": [
        "## Task 4: Train & Evaluate a Neural Network Using the Keras Library ##\n",
        "\n",
        "*Keras* is a library for implementing neural networks. It's built on top of the *TensorFlow* architecture (https://keras.io/api/)<br>\n",
        "\n",
        "Feed-forward networks are called *Sequential models* in Keras<br>\n",
        "\n",
        "Fully-connected layers (where each neuron receives input from all neurons in the previous layers) are called *Dense layers* in Keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0yf3_tuUTvb"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Run this cell to import keras modules for feed forward networks and fully connected layers\n",
        "#\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvSGW8n3f63d"
      },
      "source": [
        "**Note that**<br>\n",
        "\n",
        "a) Input layers are *implicit* in Keras. They are built with a one-to-one-correspondance to the number of features<br>\n",
        "b) `input_dim` in the first *hidden* layer specifies the number of features (`input_dim` attribute)<br><br>\n",
        "\n",
        "The model specified in the following cell consists of:<br><br>\n",
        "a) A feed-forward network<br>\n",
        "b) Three fully-connected (dense) layers<br>\n",
        "c) The two hidden layers have 5 neurons each and use the *ReLU* activation function<br>\n",
        "d) The last (output) layer has one neuron using the *sigmoid* activation function (to build a binary classifier)<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Rz1Ro1UdGS"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Create the keras model and add layers one by one, indicating the number of neurons and activation function\n",
        "#\n",
        "model = Sequential()\n",
        "#\n",
        "model.add(Dense(5, input_dim=10, activation=\"relu\"))\n",
        "model.add(Dense(5, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLgy7pQYgb13"
      },
      "source": [
        "Keras models need to be *compiled* before fitting them to the data. This is why we use the compile method in the cell below. The parameters we use are:<br>\n",
        "\n",
        "**loss**: the function that measures the error (loss)<br>\n",
        "\n",
        "**optimiser**: how the learning rate is optimised (you can read more about optimizers here: https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)<br>\n",
        "\n",
        "**metrics**: the metrics you'd like to see as part of the output during training (see below: we've selected AUC and you can see that AUC is produced at every epoch)<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl4b9SDSUhbo"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Run this cell to compile the model\n",
        "#\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"AUC\"])\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3OYuh7C8NGF"
      },
      "source": [
        "The cell below fits the compiled Keras model to the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzoCAOxRUkC0"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Run this cell to fit the model\n",
        "#\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=10)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3IoRpOt8NGF"
      },
      "source": [
        "Run the next cell the use the model to predict results for the test set, and obtain performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ViGFIXfUoFb"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Performance on test set\n",
        "#\n",
        "predictions = model.predict(X_test)\n",
        "#\n",
        "import sklearn.metrics as metrics\n",
        "#\n",
        "fpr, tpr, threshold = metrics.roc_curve(y_test, predictions)\n",
        "#\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "#\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D0zgRjvLTV8"
      },
      "source": [
        "### Some questions for you to consider ###\n",
        "\n",
        "How do you choose the number of layers and neurons per layer?\n",
        "\n",
        "\n",
        "How do you choose the numberr of training epochs?\n",
        "\n",
        "You could try different combinations of number of layers and size of each layer, e.g. Only one hidden layer with 12 neurons.\n",
        "\n",
        "**Note**: Start by running on only 5 epochs to get an idea of how the network performs\n",
        "\n",
        "Is the network performace good? How do you explain the performance?\n",
        "\n",
        "Is there any point in applying cross validation? Why?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}